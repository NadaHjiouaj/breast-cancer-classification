{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02813308",
   "metadata": {},
   "source": [
    "# Projet PDL – Classification d'images mammaires (BUSI)\n",
    "\n",
    "Ce notebook propose une implémentation **complète** et **commentée** de bout en bout d’un pipeline de classification\n",
    "d’images mammaires issues du dataset **BUSI**.  \n",
    "L’objectif est de discriminer trois classes :\n",
    "\n",
    "1. **Benign** – Tumeurs bénignes  \n",
    "2. **Malignant** – Tumeurs malignes  \n",
    "3. **Normal** – Tissu sain  \n",
    "\n",
    "Chaque section du notebook est introduite par un bloc *Markdown* détaillant les choix méthodologiques,\n",
    "puis suivie par le code correspondant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31277ff5",
   "metadata": {},
   "source": [
    "## 1. Configuration générale\n",
    "\n",
    "Dans cette section :\n",
    "\n",
    "* Définition des hyper‑paramètres dans une classe `Args` pour un accès centralisé.  \n",
    "* Initialisation des *seeds* pour assurer la **reproductibilité** des expériences sur `torch`, `numpy` et `random`.  \n",
    "* Les chemins par défaut supposent l’arborescence :\n",
    "\n",
    "```\n",
    "PDL/\n",
    " └── Dataset/\n",
    "     ├── benign/\n",
    "     ├── malignant/\n",
    "     └── normal/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Projet PDL - Classification d'images mammaires BUSI complet et fonctionnel\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import time\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "class Args:\n",
    "    data_dir = \"PDL/Dataset\"\n",
    "    model_name = \"resnet\"\n",
    "    num_classes = 3\n",
    "    epochs = 20\n",
    "    batch_size = 32\n",
    "    lr = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    dropout_rate = 0.85\n",
    "    patience = 7\n",
    "    seed = 42\n",
    "    mixup_alpha = 0.3\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Initialisation des seeds\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1747a",
   "metadata": {},
   "source": [
    "## 2. Analyse exploratoire rapide du dataset\n",
    "\n",
    "Le but est de :\n",
    "\n",
    "* **Compter** les images par classe afin d’identifier un éventuel déséquilibre.  \n",
    "* Mesurer les **dimensions** moyennes, min et max pour choisir une taille de redimensionnement appropriée.  \n",
    "* Visualiser :\n",
    "  * La répartition des classes (barplot)  \n",
    "  * La dispersion des dimensions brutes (scatter)  \n",
    "  * Le ratio largeur/hauteur (histogramme)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ce142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset_path):\n",
    "    \"\"\"Affiche quelques statistiques et visualisations de base.\"\"\"\n",
    "    print(\"\\n=== Analyse initiale du dataset ===\")\n",
    "\n",
    "    class_counts = Counter()\n",
    "    sizes = []\n",
    "\n",
    "    for cls in ['benign', 'malignant', 'normal']:\n",
    "        cls_dir = os.path.join(dataset_path, cls)\n",
    "        if not os.path.exists(cls_dir):\n",
    "            continue\n",
    "\n",
    "        for img_name in os.listdir(cls_dir):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(cls_dir, img_name)\n",
    "                with Image.open(img_path) as img:\n",
    "                    sizes.append(img.size)\n",
    "                class_counts[cls] += 1\n",
    "\n",
    "    # Affichage texte\n",
    "    total_imgs = sum(class_counts.values())\n",
    "    print(f\"\\nNombre total d'images : {total_imgs}\")\n",
    "    print(\"Répartition par classe :\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"- {cls}: {count} images ({count/total_imgs:.1%})\")\n",
    "\n",
    "    widths, heights = zip(*sizes)\n",
    "    print(f\"\\nDimensions moyennes : {np.mean(widths):.0f}×{np.mean(heights):.0f}\")\n",
    "    print(f\"Dimensions min : {min(widths)}×{min(heights)}\")\n",
    "    print(f\"Dimensions max : {max(widths)}×{max(heights)}\")\n",
    "\n",
    "    # Visualisations\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
    "    plt.title('Répartition des classes')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(widths, heights, alpha=0.5)\n",
    "    plt.xlabel('Largeur (px)')\n",
    "    plt.ylabel('Hauteur (px)')\n",
    "    plt.title('Distribution des dimensions')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist([w/h for w, h in sizes], bins=20)\n",
    "    plt.xlabel('Ratio largeur/hauteur')\n",
    "    plt.title('Ratios largeur/hauteur')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fcbb68",
   "metadata": {},
   "source": [
    "## 3. Pipelines de pré‑traitement et d’*augmentation* des données\n",
    "\n",
    "* **Entraînement** : redimensionnement vers 256², flips horizontaux/verticaux, *affine*, *ColorJitter*,\n",
    "  *RandomErasing*, puis normalisation ImageNet.  \n",
    "* **Validation / Test** : redimensionnement fixe 224² + normalisation.  \n",
    "Les mêmes statistiques sont utilisées pour la normalisation que celles d’ImageNet,\n",
    "car nous ré‑utilisons des poids pré‑entraînés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa1ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.85, 1.15), shear=5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53defb",
   "metadata": {},
   "source": [
    "## 4. Classe `BUSIDataset`\n",
    "\n",
    "Cette classe hérite de `torch.utils.data.Dataset` et :\n",
    "\n",
    "* Charge les chemins d’images et leur étiquette **une seule fois** lors de l’initialisation.  \n",
    "* Convertit chaque image en RGB, puis applique la **transformation** appropriée (train/val/test).  \n",
    "* Expose `classes` et `class_to_idx` pour un accès simple aux noms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d030af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BUSIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.classes = ['benign', 'malignant', 'normal']\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        for cls in self.classes:\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "\n",
    "            for img_name in os.listdir(cls_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(cls_dir, img_name), self.class_to_idx[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ae795",
   "metadata": {},
   "source": [
    "## 5. Architecture du modèle\n",
    "\n",
    "Nous utilisons **ResNet‑18** pré‑entraînée ImageNet :\n",
    "\n",
    "* **Gel** des couches jusqu’à `layer3` incluse pour réduire le coût d’entraînement et éviter le sur‑apprentissage.  \n",
    "* Remplacement de la *fully‑connected* par une tête sur‑mesure :\n",
    "  * `Dropout` (réduire overfitting)  \n",
    "  * `Linear` → 128\n",
    "  * `BatchNorm1d` + `ReLU`\n",
    "  * `Dropout`\n",
    "  * `Linear` final vers `num_classes`  \n",
    "* `dropout_rate` configurable dans `Args`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f55dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # Gel des couches basses\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    # Tête de classification\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(args.dropout_rate),\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, args.num_classes)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a388071",
   "metadata": {},
   "source": [
    "## 6. Augmentation **Mixup**\n",
    "\n",
    "*Mixup* crée des exemples synthétiques en mélangeant deux images et leurs labels\n",
    "selon un coefficient `λ ~ Beta(α, α)`.\n",
    "\n",
    "Avantages :\n",
    "\n",
    "* Lisse la frontière de décision.  \n",
    "* Améliore la généralisation surtout sur de petits datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c110e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.4):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28718a7a",
   "metadata": {},
   "source": [
    "## 7. Fonctions de visualisation des métriques\n",
    "\n",
    "* **Courbes d’apprentissage** : loss, accuracy, learning rate.  \n",
    "* **Matrice de confusion** normalisée.  \n",
    "* **Analyse d’erreurs** : affichage des images mal classées avec probas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['epoch'], history['train_loss'], label='Train')\n",
    "    plt.plot(history['epoch'], history['val_loss'], label='Validation')\n",
    "    plt.title('Évolution de la Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['epoch'], history['train_acc'], label='Train')\n",
    "    plt.plot(history['epoch'], history['val_acc'], label='Validation')\n",
    "    plt.title('Évolution de l\\'Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['epoch'], history['lr'])\n",
    "    plt.title('Évolution du Learning Rate')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Matrice de Confusion Normalisée')\n",
    "    plt.xlabel('Prédictions')\n",
    "    plt.ylabel('Vraies étiquettes')\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_analysis(model, test_loader, device, class_names):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                if preds[i] != labels[i]:\n",
    "                    errors.append((\n",
    "                        inputs[i].cpu(),\n",
    "                        labels[i].cpu(),\n",
    "                        preds[i].cpu(),\n",
    "                        F.softmax(outputs[i], dim=0).cpu().numpy()\n",
    "                    ))\n",
    "            if len(errors) >= 8:\n",
    "                break\n",
    "\n",
    "    if not errors:\n",
    "        print(\"Aucune erreur à afficher!\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    for i in range(min(8, len(errors))):\n",
    "        img, true, pred, probs = errors[i]\n",
    "\n",
    "        # Image\n",
    "        plt.subplot(8, 2, 2*i+1)\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Vrai: {class_names[true]}\\nPrédit: {class_names[pred]}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Probabilités\n",
    "        plt.subplot(8, 2, 2*i+2)\n",
    "        plt.barh(class_names, probs)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.xlabel('Probabilité')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c4fd85",
   "metadata": {},
   "source": [
    "## 8. Boucle d’entraînement, validation et sauvegarde\n",
    "\n",
    "Points clés :\n",
    "\n",
    "* **Scheduler** : `ReduceLROnPlateau` (divise LR par 2 si la val‑accuracy stagne 3 epochs).  \n",
    "* **Label Smoothing** : `CrossEntropyLoss(label_smoothing=0.2)` atténue la sur‑confiance.  \n",
    "* **Early Stopping** : patience configurable (`Args.patience`).  \n",
    "* Sauvegarde automatique du *meilleur* modèle dans `best_model.pth`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate():\n",
    "    # 1) Analyse exploratoire\n",
    "    analyze_dataset(args.data_dir)\n",
    "\n",
    "    # 2) Création des jeux Train / Val / Test\n",
    "    dataset = BUSIDataset(args.data_dir, transform=train_transform)\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size   = int(0.15 * len(dataset))\n",
    "    test_size  = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(args.seed)\n",
    "    )\n",
    "\n",
    "    # Switch des transforms pour val/test\n",
    "    val_dataset.dataset.transform  = val_transform\n",
    "    test_dataset.dataset.transform = val_transform\n",
    "\n",
    "    # 3) DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=args.batch_size, num_workers=2)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=args.batch_size, num_workers=2)\n",
    "\n",
    "    # 4) Modèle\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nUtilisation de {device}\")\n",
    "    model = get_model().to(device)\n",
    "\n",
    "    # 5) Optimisation\n",
    "    optimizer  = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler  = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    criterion  = nn.CrossEntropyLoss(label_smoothing=0.2)\n",
    "\n",
    "    # 6) Historique\n",
    "    history = {'epoch': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    print(\"\\nDébut de l'entraînement...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Mixup\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, args.mixup_alpha)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total   += labels.size(0)\n",
    "            correct += lam * (predicted == targets_a).sum().item() + (1 - lam) * (predicted == targets_b).sum().item()\n",
    "\n",
    "        # Train metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc  = 100 * correct / total\n",
    "\n",
    "        # Validation metrics\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Historique\n",
    "        history['epoch'].append(epoch+1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs} - Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}% | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == args.patience:\n",
    "                print(f\"\\nEarly stopping à l'époque {epoch+1}!\")\n",
    "                break\n",
    "\n",
    "    # Visualisation des courbes\n",
    "    plot_learning_curves(history)\n",
    "\n",
    "    # Évaluation finale\n",
    "    print(\"\\n=== Évaluation sur le test set ===\")\n",
    "    evaluate_model(model, test_loader, device, dataset.classes)\n",
    "\n",
    "    print(f\"\\nTemps total: {(time.time()-start_time)/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38a044",
   "metadata": {},
   "source": [
    "## 9. Fonctions `validate` et `evaluate_model`\n",
    "\n",
    "* `validate` : boucle simple sans gradient, renvoie loss & accuracy sur le set de validation.  \n",
    "* `evaluate_model` : recharge **le meilleur** modèle, calcule *accuracy*, *F1 macro*, matrice de confusion et lance l’analyse d’erreurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total   += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return val_loss / len(val_loader), 100 * correct / total\n",
    "\n",
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Évaluation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
    "    print(f\"F1-Score (macro): {f1:.4f}\")\n",
    "\n",
    "    plot_confusion_matrix(all_labels, all_preds, class_names)\n",
    "    print(\"\\n=== Rapport de classification ===\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
    "\n",
    "    print(\"\\n=== Analyse des erreurs ===\")\n",
    "    plot_error_analysis(model, test_loader, device, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652668d1",
   "metadata": {},
   "source": [
    "## 10. Point d’entrée\n",
    "\n",
    "Exécute le pipeline complet (`train_and_validate`) si le notebook est lancé comme script.  \n",
    "Pour un usage interactif, il suffit d’appeler `train_and_validate()` depuis une cellule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_and_validate()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
